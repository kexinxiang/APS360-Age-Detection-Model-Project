{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH4-jDPZ3PI9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Project Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhrjac4V_K8T"
      },
      "source": [
        "# Part 1. Data Processing\n",
        "The following procedures demonstrate how the data for this project is collected, cleaned, and processed.\n",
        "\n",
        "*   The data is downloaded from Kaggle.com, with the data seperated into 44 folders from the age range of 16 - 60.\n",
        "\n",
        "*   The data is then sorted according to the purpose of our project.\n",
        "For the age range 6- 60, the data is organized into the folder called \"below_16\", while for the age range 17-60, the data is organized into the folder named \"under_16\".\n",
        "\n",
        "*   The data is then retrieved from these folders and splits into training, validation, and testing set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dkMlWqLsMJO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "\n",
        "use_cuda=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dv8nwRSrZZY",
        "outputId": "45f5c622-8c4a-44e0-d979-d8ae81413107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoDpDUdwR3JN"
      },
      "source": [
        "### Part (b) Splitting data to training validation and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the data will be randomized and split into 3 sets: 75% into training set, 12.5% into validation set, and 12.5% into test set. The cleaned data will be split and stored into the folder caleed \"cleaned_data\"."
      ],
      "metadata": {
        "id": "OpREmFhJ6AIz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VEaWawZNUo1"
      },
      "outputs": [],
      "source": [
        "cleaned_dataset_path = \"/content/gdrive/MyDrive/APS360 group project/cleaned_dataset\"\n",
        "cleaned_dataset = torchvision.datasets.DatasetFolder(cleaned_dataset_path, loader=torch.load, extensions=('.tensor'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6muSmOY9fLk"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.75 * len(cleaned_dataset))\n",
        "val_size = int(0.125 * len(cleaned_dataset))\n",
        "test_size = len(cleaned_dataset) - train_size - val_size\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(\n",
        "    cleaned_dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDmITqTrLCFm"
      },
      "outputs": [],
      "source": [
        "b_s = 16\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=b_s,\n",
        "                                           num_workers=1, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=b_s,\n",
        "                                           num_workers=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=b_s,\n",
        "                                           num_workers=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV0V-ZFmrq8T"
      },
      "source": [
        "# Part 2. Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (a) Building baseline model\n"
      ],
      "metadata": {
        "id": "Ffaw4qME6nfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Description of model, paste from progress report)"
      ],
      "metadata": {
        "id": "9lUsU77cjY5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is another CNN baseline model with more advanced feature extracting ability.\n",
        "\n",
        "(Description, paste from progress report)"
      ],
      "metadata": {
        "id": "DgRloZpAjx-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class primary_CNN_features_1(nn.Module):\n",
        "  def __init__(self):\n",
        "        self.name='CNN'\n",
        "        super(primary_CNN_features_1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3,8,7) # in_channels, out_chanels, kernel_size  # Stride defalt to be 1, padding 0\n",
        "        self.conv2 = nn.Conv2d(8,16,5)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 4, 2)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(123904, 1280)\n",
        "        self.fc2 = nn.Linear(1280, 320)\n",
        "        self.fc3 = nn.Linear(320, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x= self.pool(F.relu(self.conv3(x)))\n",
        "      x = x.view(-1, 123904)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "R42YTMKAjR5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (b) Useful functions"
      ],
      "metadata": {
        "id": "TMFe8wcu6raK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section provides several useful function for training and reporting accuracy."
      ],
      "metadata": {
        "id": "ofp_XGIN6wOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29nTyXJab6lZ"
      },
      "outputs": [],
      "source": [
        "def train_CNN(model, train_loader, val_loader, batch_size=32, l_r=0.005, num_epochs=5, momentum = 0.4):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=l_r, momentum=momentum)\n",
        "\n",
        "    iters, train_losses, val_losses, train_acc, val_acc = [], [], [], [], []\n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        for imgs, labels in iter(train_loader):\n",
        "\n",
        "\n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            #############################################\n",
        "\n",
        "\n",
        "            out = model(imgs)             # forward pass\n",
        "            train_loss = criterion(out, labels) # compute the total loss\n",
        "            train_loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "        iters.append(n)\n",
        "        train_losses.append(float(train_loss)/batch_size)             # compute *average* loss\n",
        "\n",
        "\n",
        "        for imgs, labels in iter(val_loader):\n",
        "\n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            #############################################\n",
        "\n",
        "\n",
        "            out = model(imgs)             # forward pass\n",
        "            val_loss = criterion(out, labels) # compute the total loss\n",
        "\n",
        "        val_losses.append(float(val_loss)/batch_size)             # compute *average* loss\n",
        "\n",
        "        train_acc.append(get_accuracy(model, train_loader, val_loader, train=True)) # compute training accuracy\n",
        "        val_acc.append(get_accuracy(model, train_loader, val_loader, train=False))  # compute validation accuracy\n",
        "        model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name, batch_size, l_r, epoch)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        n+=1\n",
        "\n",
        "        print((\"Epoch: {} | Training Accuracy: {} | Validation Accuracy: {}\").format(epoch+1, train_acc[epoch], val_acc[epoch]))\n",
        "\n",
        "    plot(iters, train_losses, val_losses, train_acc, val_acc)\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_CNN_2(model, train_set, val_set, batch_size=32, l_r=0.005, num_epochs=5, momentum=0.4):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=l_r, momentum=momentum)\n",
        "\n",
        "    iters, train_losses, val_losses, train_acc, val_acc = [], [], [], [], []\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                           num_workers=1, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                           num_workers=1, shuffle=True)\n",
        "\n",
        "    # training\n",
        "    n = 0  # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        total_train_loss = 0.0  # Variable to store the total training loss for this epoch\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "                imgs = imgs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(imgs)  # forward pass\n",
        "            train_loss = criterion(out, labels)  # compute the total loss\n",
        "            train_loss.backward()  # backward pass (compute parameter updates)\n",
        "            optimizer.step()  # make the updates for each parameter\n",
        "\n",
        "            total_train_loss += train_loss.item()  # Accumulate the loss for this batch\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        total_val_loss = 0.0  # Variable to store the total validation loss for this epoch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                if use_cuda and torch.cuda.is_available():\n",
        "                    imgs = imgs.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                out = model(imgs)  # forward pass\n",
        "                val_loss = criterion(out, labels)  # compute the total loss\n",
        "                total_val_loss += val_loss.item()  # Accumulate the loss for this batch\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_acc_epoch = get_accuracy(model, train_loader)\n",
        "        val_acc_epoch = get_accuracy(model, val_loader)\n",
        "        train_acc.append(train_acc_epoch)\n",
        "        val_acc.append(val_acc_epoch)\n",
        "\n",
        "        model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name, batch_size, l_r, epoch)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        n += 1\n",
        "\n",
        "        print((\"Epoch: {} | Training Loss: {:.4f} | Validation Loss: {:.4f} | \"\n",
        "               \"Training Accuracy: {:.4f} | Validation Accuracy: {:.4f}\").format(\n",
        "            epoch + 1, avg_train_loss, avg_val_loss, train_acc_epoch, val_acc_epoch\n",
        "        ))\n",
        "\n",
        "    plot(iters, train_losses, val_losses, train_acc, val_acc)\n",
        "\n",
        "    print(\"Final Training Accuracy: {:.4f}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ],
      "metadata": {
        "id": "TqmlKVPOBrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(iters, train_losses, val_losses, train_acc, val_acc):\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_losses, label=\"Train\")\n",
        "    plt.plot(iters, val_losses, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kcQe9knf68w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "17YTQv4l54W1"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model,train_loader,val_loader,train=False):\n",
        "    if train:\n",
        "        data = train_loader\n",
        "    else:\n",
        "        data = val_loader\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in data:\n",
        "\n",
        "\n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          imgs = imgs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        #############################################\n",
        "\n",
        "\n",
        "        output = model(imgs)\n",
        "\n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSObOcodHlQ0"
      },
      "outputs": [],
      "source": [
        "def get_test_accuracy(model, data, batch_size):\n",
        "\n",
        "    data = test_set\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=batch_size):\n",
        "\n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          imgs = imgs.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        #############################################\n",
        "\n",
        "        output = model(imgs)\n",
        "\n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0JeaN3_HMmM"
      },
      "source": [
        "### Part (c) Training baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will train our second CNN model named `primary_CNN_features_1` with learning rate of 1e-4 and number of epochs = 30, batch_size = 32.\n",
        "\n"
      ],
      "metadata": {
        "id": "b5vvbHMgmVYQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_iw4D41m84A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f660f19d-6b3b-49e0-eee6-06363434174b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "model_2 = primary_CNN_features_1()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_2.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_CNN_2(model_2, train_set, val_set, l_r=2e-4,num_epochs=10, momentum = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "IhEp6agP42Vp",
        "outputId": "005c0399-95c4-4127-b101-268e22dd6d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7f014084317b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_CNN_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-3fbff4b6901f>\u001b[0m in \u001b[0;36mtrain_CNN_2\u001b[0;34m(model, train_set, val_set, batch_size, l_r, num_epochs, momentum)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# compute the total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backward pass (compute parameter updates)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make the updates for each parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (32)."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "pRkV4mNdqGqA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}